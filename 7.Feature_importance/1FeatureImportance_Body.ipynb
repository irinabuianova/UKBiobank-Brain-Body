{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80e4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "from datetime import datetime\n",
    "from matplotlib_venn import venn2\n",
    "from scipy.stats import pearsonr\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27308927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modality renaming dictionary (modality_map)\n",
    "modality_map = {\n",
    "'hearing': 'Hearing',\n",
    "'immune': 'Immune',\n",
    "'renalhepatic': 'Renal & Hepatic',\n",
    "'metabolic': 'Metabolic',\n",
    "'cardiopulmonary': 'Cardiopulmonary',\n",
    "'musculoskeletal': 'Musculoskeletal',\n",
    "'bone_densitometry': 'Bone Densitometry of Heel',\n",
    "'pwa': 'Pulse Wave Analysis',\n",
    "'heart_mri': 'Heart MRI',\n",
    "'carotid_ultrasound': 'Carotid Ultrasound',\n",
    "'arterial_stiffness': 'Arterial Stiffness',\n",
    "'ecg_rest': 'ECG at Rest',\n",
    "'body_composition_by_impedance': 'Body Composition by Impedance',\n",
    "'body_composition_dxa': 'Body Composition by DXA',\n",
    "'bone_dxa': 'Bone Size, Mineral and Density by DXA',\n",
    "'kidneys_mri': 'Kidney MRI',\n",
    "'liver_mri': 'Liver MRI',\n",
    "'abdominal_composition_mri_18_vars': 'Abdominal Composition by MRI',\n",
    "'abdominal_organ_composition_mri_13_vars': 'Abdominal Organ Composition by MRI',\n",
    "'struct_fast' : 'Regional grey matter volumes (FSL FAST)',\n",
    "'struct_sub_first': 'Subcortical volumes (FSL FIRST)',\n",
    "\n",
    "'struct_fs_aseg_mean_intensity' : 'ASEG Mean Intensity',\n",
    "'struct_fs_aseg_volume' : 'ASEG Volume',\n",
    "\n",
    "\n",
    "'struct_ba_exvivo_area' : 'BA ex-vivo Area',\n",
    "'struct_ba_exvivo_mean_thickness' : 'BA ex-vivo Mean Thickness',\n",
    "'struct_ba_exvivo_volume' : 'BA ex-vivo Volume',\n",
    "\n",
    "'struct_a2009s_area' : 'a2009s Area',\n",
    "'struct_a2009s_mean_thickness' : 'a2009s Mean Thickness',\n",
    "'struct_a2009s_volume' : 'a2009s Volume',\n",
    "\n",
    "\n",
    "'struct_dkt_area' : 'Desikan-Killiany-Tourville Area',\n",
    "'struct_dkt_mean_thickness' : 'Desikan-Killiany-Tourville Mean Thickness',\n",
    "'struct_dkt_volume' : 'Desikan-Killiany-Tourville Volume',\n",
    "\n",
    "\n",
    "'struct_desikan_gw' : 'Desikan Grey/White Matter Contrast',\n",
    "'struct_desikan_pial' : 'Desikan Pial',\n",
    "\n",
    "'struct_desikan_white_area' : 'Desikan White Matter Area',\n",
    "'struct_desikan_white_mean_thickness' : 'Desikan White Matter Mean Thickness',\n",
    "'struct_desikan_white_volume' : 'Desikan White Matter Volume',\n",
    "\"struct_subsegmentation\":'Subcortical Volumetric Subsegmentation',\n",
    "\n",
    "'add_t1' : 'Whole-Brain T1w',\n",
    "'add_t2' : 'Whole-Brain T2w',\n",
    "\"dwi_FA_tbss\": \"FA TBSS\",\n",
    "\"dwi_FA_prob\": \"FA Probabilistic\",\n",
    "\"dwi_MD_tbss\": \"MD TBSS\",\n",
    "\"dwi_MD_prob\": \"MD Probabilistic\",\n",
    "\"dwi_L1_tbss\": \"L1 TBSS\",\n",
    "\"dwi_L1_prob\": \"L1 Probabilistic\",\n",
    "\"dwi_L2_tbss\": \"L2 TBSS\",\n",
    "\"dwi_L2_prob\": \"L2 Probabilistic\",\n",
    "\"dwi_L3_tbss\": \"L3 TBSS\",\n",
    "\"dwi_L3_prob\": \"L3 Probabilistic\",\n",
    "\"dwi_MO_tbss\": \"MO TBSS\",\n",
    "\"dwi_MO_prob\": \"MO Probabilistic\",\n",
    "\"dwi_OD_tbss\": \"OD TBSS\",\n",
    "\"dwi_OD_prob\": \"OD Probabilistic\",\n",
    "\"dwi_ICVF_tbss\": \"ICVF TBSS\",\n",
    "\"dwi_ICVF_prob\": \"ICVF Probabilistic\",\n",
    "\"dwi_ISOVF_tbss\": \"ISOVF TBSS\",\n",
    "\"dwi_ISOVF_prob\": 'ISOVF Probabilistic',\n",
    "\"amplitudes_21\": \" 21 IC Amplitudes\",\n",
    "\"amplitudes_55\": \"55 IC Amplitudes\",\n",
    "\"full_correlation_21\": \"21 IC Full Correlation\",\n",
    "\"full_correlation_55\": \"55 IC Full Correlation\",\n",
    "\"partial_correlation_21\": \" 21 IC Partial Correlation\",\n",
    "\"partial_correlation_55\": \" 55 IC Partial Correlation\",\n",
    "# aparc Tian S1 (I)\n",
    "'aparc_Tian_S1_FA_i2': 'aparc-I FA',\n",
    "'aparc_Tian_S1_Length_i2': 'aparc-I Length',\n",
    "'aparc_Tian_S1_SIFT2_FBC_i2': 'aparc-I SIFT2 FBC',\n",
    "'aparc_Tian_S1_Streamline_Count_i2': 'aparc-I Streamline Count',\n",
    "\n",
    "# aparc a2009s Tian S1 (I)\n",
    "'aparc_a2009s_Tian_S1_FA_i2': 'aparc.a2009s-I FA',\n",
    "'aparc_a2009s_Tian_S1_Length_i2': 'aparc.a2009s-I Length',\n",
    "'aparc_a2009s_Tian_S1_SIFT2_FBC_i2': 'aparc.a2009s-I SIFT2 FBC',\n",
    "'aparc_a2009s_Tian_S1_Streamline_Count_i2': 'aparc.a2009s-I Streamline Count',\n",
    "\n",
    "# Glasser Tian S1 (I)\n",
    "'Glasser_Tian_S1_FA_i2': 'Glasser-I FA',\n",
    "'Glasser_Tian_S1_Length_i2': 'Glasser-I Length',\n",
    "'Glasser_Tian_S1_SIFT2_FBC_i2': 'Glasser-I SIFT2 FBC',\n",
    "'Glasser_Tian_S1_Streamline_Count_i2': 'Glasser-I Streamline Count',\n",
    "\n",
    "# Glasser Tian S4 (IV)\n",
    "'Glasser_Tian_S4_FA_i2': 'Glasser-IV FA',\n",
    "'Glasser_Tian_S4_Length_i2': 'Glasser-IV Length',\n",
    "'Glasser_Tian_S4_SIFT2_FBC_i2': 'Glasser-IV SIFT2 FBC',\n",
    "'Glasser_Tian_S4_Streamline_Count_i2': 'Glasser-IV Streamline Count',\n",
    "\n",
    "# Schaefer7n1000p Tian S4 (IV) (in reality: Schaefer7n200p Tian S1)\n",
    "'Schaefer7n1000p_Tian_S4_FA_i2': 'Schaefer7n200p-I FA', #'Schaefer7n1000p-IV FA',\n",
    "'Schaefer7n1000p_Tian_S4_Length_i2': 'Schaefer7n200p-I Length',#'Schaefer7n1000p-IV Length',\n",
    "'Schaefer7n1000p_Tian_S4_SIFT2_FBC_i2': 'Schaefer7n200p-I SIFT2 FBC',#'Schaefer7n1000p-IV SIFT2 FBC',\n",
    "'Schaefer7n1000p_Tian_S4_Streamline_Count_i2': 'Schaefer7n200p-I Streamline Count', #'Schaefer7n1000p-IV Streamline Count'\n",
    "\n",
    "# Schaefer7n200p Tian S4 (IV) (in reality: Schaefer7n500p Tian S4)\n",
    "'Schaefer7n200p_Tian_S1_FA_i2': 'Schaefer7n500p-IV FA',\n",
    "'Schaefer7n200p_Tian_S1_Length_i2': 'Schaefer7n500p-IV Length',\n",
    "'Schaefer7n200p_Tian_S1_SIFT2_FBC_i2': 'Schaefer7n500p-IV SIFT2 FBC',\n",
    "'Schaefer7n200p_Tian_S1_Streamline_Count_i2': 'Schaefer7n500p-IV Streamline Count',\n",
    "\n",
    "# Schaefer7n500p Tian S4 (IV) (in reality: Schaefer7n1000p Tian S4)\n",
    "'Schaefer7n500p_Tian_S4_FA_i2': 'Schaefer7n1000p-IV FA',\n",
    "'Schaefer7n500p_Tian_S4_Length_i2': 'Schaefer7n1000p-IV Length',\n",
    "'Schaefer7n500p_Tian_S4_SIFT2_FBC_i2': 'Schaefer7n1000p-IV SIFT2 FBC',\n",
    "'Schaefer7n500p_Tian_S4_Streamline_Count_i2': 'Schaefer7n1000p-IV Streamline Count',\n",
    "\n",
    "# Resting state \n",
    "'full_correlation_aparc_a2009s_Tian_S1' : 'aparc.a2009s-I Full Correlation',\n",
    "'full_correlation_aparc_Tian_S1': 'aparc-I Full Correlation',\n",
    "'full_correlation_Glasser_Tian_S1': 'Glasser-I Full Correlation',\n",
    "'full_correlation_Glasser_Tian_S4': 'Glasser-IV Full Correlation',\n",
    "'full_correlation_Schaefer7n200p_Tian_S1': 'Schaefer7n200p-I Full Correlation',\n",
    "'full_correlation_Schaefer7n500p_Tian_S4': 'Schaefer7n500p-IV Full Correlation',\n",
    "'partial_correlation_aparc_a2009s_Tian_S1': 'aparc.a2009s-I Partial Correlation',\n",
    "'partial_correlation_aparc_Tian_S1': 'aparc-I Partial Correlation',\n",
    "'partial_correlation_Glasser_Tian_S1': 'Glasser-I Partial Correlation',\n",
    "'partial_correlation_Glasser_Tian_S4': 'Glasser-IV Partial Correlation',\n",
    "'partial_correlation_Schaefer7n200p_Tian_S1': 'Schaefer7n200p-I Partial Correlation',\n",
    "'partial_correlation_Schaefer7n500p_Tian_S4': 'Schaefer7n500p-IV Partial Correlation',\n",
    "\n",
    "'lifestyle-envir': 'Lifestyle & Environment',\n",
    "\n",
    "'allmri': '3 Brain MRI Modalities Stacked',\n",
    "'dwi': 'Brain dwMRI Stacked',\n",
    "'smri': 'Brain sMRI Stacked',\n",
    "'rs': 'Brain rsMRI Stacked',\n",
    "'body': 'Body Physiology Stacked',\n",
    "'body-comp': 'Body Composition Stacked',\n",
    "#'cardiopulmonary': 'Cardiopulmonary stacked',\n",
    "'renal-hepatic': 'Renal & Hepatic Stacked',\n",
    "'lifestyle-envir': 'Lifestyle & Environment',\n",
    "'brain-plus-body': '3 Brain MRI Modalities & Body Stacked',\n",
    "'brain-body': 'Brain & Body Stacked',\n",
    "'body-only': 'Body Physiology and Composition Stacked' \n",
    "}\n",
    "\n",
    "# Define modality names for renaming\n",
    "modality_names = {\n",
    "'hearing': 'Hearing',\n",
    "'immune': 'Immune',\n",
    "'renalhepatic': 'Renal & Hepatic',\n",
    "'metabolic': 'Metabolic',\n",
    "'cardiopulmonary': 'Cardiopulmonary',\n",
    "'musculoskeletal': 'Musculoskeletal',\n",
    "'bone_densitometry': 'Bone Densitometry of Heel',\n",
    "'pwa': 'Pulse Wave Analysis',\n",
    "'heart_mri': 'Heart MRI',\n",
    "'carotid_ultrasound': 'Carotid Ultrasound',\n",
    "'arterial_stiffness': 'Arterial Stiffness',\n",
    "'ecg_rest': 'ECG at Rest',\n",
    "'body_composition_by_impedance': 'Body Composition by Impedance',\n",
    "'body_composition_dxa': 'Body Composition by DXA',\n",
    "'bone_dxa': 'Bone Size, Mineral and Density by DXA',\n",
    "'kidneys_mri': 'Kidney MRI',\n",
    "'liver_mri': 'Liver MRI',\n",
    "'abdominal_composition_mri_18_vars': 'Abdominal Composition by MRI',\n",
    "'abdominal_organ_composition_mri_13_vars': 'Abdominal Organ Composition by MRI',\n",
    "'struct_fast' : 'Regional grey matter volumes (FSL FAST)',\n",
    "'struct_sub_first': 'Subcortical volumes (FSL FIRST)',\n",
    "\n",
    "'struct_fs_aseg_mean_intensity' : 'ASEG Mean Intensity',\n",
    "'struct_fs_aseg_volume' : 'ASEG Volume',\n",
    "\n",
    "\n",
    "'struct_ba_exvivo_area' : 'BA ex-vivo Area',\n",
    "'struct_ba_exvivo_mean_thickness' : 'BA ex-vivo Mean Thickness',\n",
    "'struct_ba_exvivo_volume' : 'BA ex-vivo Volume',\n",
    "\n",
    "'struct_a2009s_area' : 'a2009s Area',\n",
    "'struct_a2009s_mean_thickness' : 'a2009s Mean Thickness',\n",
    "'struct_a2009s_volume' : 'a2009s Volume',\n",
    "\n",
    "\n",
    "'struct_dkt_area' : 'Desikan-Killiany-Tourville Area',\n",
    "'struct_dkt_mean_thickness' : 'Desikan-Killiany-Tourville Mean Thickness',\n",
    "'struct_dkt_volume' : 'Desikan-Killiany-Tourville Volume',\n",
    "\n",
    "\n",
    "'struct_desikan_gw' : 'Desikan Grey/White Matter Contrast',\n",
    "'struct_desikan_pial' : 'Desikan Pial',\n",
    "\n",
    "'struct_desikan_white_area' : 'Desikan White Matter Area',\n",
    "'struct_desikan_white_mean_thickness' : 'Desikan White Matter Mean Thickness',\n",
    "'struct_desikan_white_volume' : 'Desikan White Matter Volume',\n",
    "\"struct_subsegmentation\":'Subcortical Volumetric Subsegmentation',\n",
    "\n",
    "'add_t1' : 'Whole-Brain T1w',\n",
    "'add_t2' : 'Whole-Brain T2w',\n",
    "\"dwi_FA_tbss\": \"FA TBSS\",\n",
    "\"dwi_FA_prob\": \"FA Probabilistic\",\n",
    "\"dwi_MD_tbss\": \"MD TBSS\",\n",
    "\"dwi_MD_prob\": \"MD Probabilistic\",\n",
    "\"dwi_L1_tbss\": \"L1 TBSS\",\n",
    "\"dwi_L1_prob\": \"L1 Probabilistic\",\n",
    "\"dwi_L2_tbss\": \"L2 TBSS\",\n",
    "\"dwi_L2_prob\": \"L2 Probabilistic\",\n",
    "\"dwi_L3_tbss\": \"L3 TBSS\",\n",
    "\"dwi_L3_prob\": \"L3 Probabilistic\",\n",
    "\"dwi_MO_tbss\": \"MO TBSS\",\n",
    "\"dwi_MO_prob\": \"MO Probabilistic\",\n",
    "\"dwi_OD_tbss\": \"OD TBSS\",\n",
    "\"dwi_OD_prob\": \"OD Probabilistic\",\n",
    "\"dwi_ICVF_tbss\": \"ICVF TBSS\",\n",
    "\"dwi_ICVF_prob\": \"ICVF Probabilistic\",\n",
    "\"dwi_ISOVF_tbss\": \"ISOVF TBSS\",\n",
    "\"dwi_ISOVF_prob\": 'ISOVF Probabilistic',\n",
    "\"amplitudes_21\": \" 21 IC Amplitudes\",\n",
    "\"amplitudes_55\": \"55 IC Amplitudes\",\n",
    "\"full_correlation_21\": \"21 IC Full Correlation\",\n",
    "\"full_correlation_55\": \"55 IC Full Correlation\",\n",
    "\"partial_correlation_21\": \" 21 IC Partial Correlation\",\n",
    "\"partial_correlation_55\": \" 55 IC Partial Correlation\",\n",
    "# aparc Tian S1 (I)\n",
    "'aparc_Tian_S1_FA_i2': 'aparc-I FA',\n",
    "'aparc_Tian_S1_Length_i2': 'aparc-I Length',\n",
    "'aparc_Tian_S1_SIFT2_FBC_i2': 'aparc-I SIFT2 FBC',\n",
    "'aparc_Tian_S1_Streamline_Count_i2': 'aparc-I Streamline Count',\n",
    "\n",
    "# aparc a2009s Tian S1 (I)\n",
    "'aparc_a2009s_Tian_S1_FA_i2': 'aparc.a2009s-I FA',\n",
    "'aparc_a2009s_Tian_S1_Length_i2': 'aparc.a2009s-I Length',\n",
    "'aparc_a2009s_Tian_S1_SIFT2_FBC_i2': 'aparc.a2009s-I SIFT2 FBC',\n",
    "'aparc_a2009s_Tian_S1_Streamline_Count_i2': 'aparc.a2009s-I Streamline Count',\n",
    "\n",
    "# Glasser Tian S1 (I)\n",
    "'Glasser_Tian_S1_FA_i2': 'Glasser-I FA',\n",
    "'Glasser_Tian_S1_Length_i2': 'Glasser-I Length',\n",
    "'Glasser_Tian_S1_SIFT2_FBC_i2': 'Glasser-I SIFT2 FBC',\n",
    "'Glasser_Tian_S1_Streamline_Count_i2': 'Glasser-I Streamline Count',\n",
    "\n",
    "# Glasser Tian S4 (IV)\n",
    "'Glasser_Tian_S4_FA_i2': 'Glasser-IV FA',\n",
    "'Glasser_Tian_S4_Length_i2': 'Glasser-IV Length',\n",
    "'Glasser_Tian_S4_SIFT2_FBC_i2': 'Glasser-IV SIFT2 FBC',\n",
    "'Glasser_Tian_S4_Streamline_Count_i2': 'Glasser-IV Streamline Count',\n",
    "\n",
    "# Schaefer7n1000p Tian S4 (IV) (in reality: Schaefer7n200p Tian S1)\n",
    "'Schaefer7n1000p_Tian_S4_FA_i2': 'Schaefer7n200p-I FA', #'Schaefer7n1000p-IV FA',\n",
    "'Schaefer7n1000p_Tian_S4_Length_i2': 'Schaefer7n200p-I Length',#'Schaefer7n1000p-IV Length',\n",
    "'Schaefer7n1000p_Tian_S4_SIFT2_FBC_i2': 'Schaefer7n200p-I SIFT2 FBC',#'Schaefer7n1000p-IV SIFT2 FBC',\n",
    "'Schaefer7n1000p_Tian_S4_Streamline_Count_i2': 'Schaefer7n200p-I Streamline Count', #'Schaefer7n1000p-IV Streamline Count'\n",
    "\n",
    "# Schaefer7n200p Tian S4 (IV) (in reality: Schaefer7n500p Tian S4)\n",
    "'Schaefer7n200p_Tian_S1_FA_i2': 'Schaefer7n500p-IV FA',\n",
    "'Schaefer7n200p_Tian_S1_Length_i2': 'Schaefer7n500p-IV Length',\n",
    "'Schaefer7n200p_Tian_S1_SIFT2_FBC_i2': 'Schaefer7n500p-IV SIFT2 FBC',\n",
    "'Schaefer7n200p_Tian_S1_Streamline_Count_i2': 'Schaefer7n500p-IV Streamline Count',\n",
    "\n",
    "# Schaefer7n500p Tian S4 (IV) (in reality: Schaefer7n1000p Tian S4)\n",
    "'Schaefer7n500p_Tian_S4_FA_i2': 'Schaefer7n1000p-IV FA',\n",
    "'Schaefer7n500p_Tian_S4_Length_i2': 'Schaefer7n1000p-IV Length',\n",
    "'Schaefer7n500p_Tian_S4_SIFT2_FBC_i2': 'Schaefer7n1000p-IV SIFT2 FBC',\n",
    "'Schaefer7n500p_Tian_S4_Streamline_Count_i2': 'Schaefer7n1000p-IV Streamline Count',\n",
    "\n",
    "# Resting state \n",
    "'full_correlation_aparc_a2009s_Tian_S1' : 'aparc.a2009s-I Full Correlation',\n",
    "'full_correlation_aparc_Tian_S1': 'aparc-I Full Correlation',\n",
    "'full_correlation_Glasser_Tian_S1': 'Glasser-I Full Correlation',\n",
    "'full_correlation_Glasser_Tian_S4': 'Glasser-IV Full Correlation',\n",
    "'full_correlation_Schaefer7n200p_Tian_S1': 'Schaefer7n200p-I Full Correlation',\n",
    "'full_correlation_Schaefer7n500p_Tian_S4': 'Schaefer7n500p-IV Full Correlation',\n",
    "'partial_correlation_aparc_a2009s_Tian_S1': 'aparc.a2009s-I Partial Correlation',\n",
    "'partial_correlation_aparc_Tian_S1': 'aparc-I Partial Correlation',\n",
    "'partial_correlation_Glasser_Tian_S1': 'Glasser-I Partial Correlation',\n",
    "'partial_correlation_Glasser_Tian_S4': 'Glasser-IV Partial Correlation',\n",
    "'partial_correlation_Schaefer7n200p_Tian_S1': 'Schaefer7n200p-I Partial Correlation',\n",
    "'partial_correlation_Schaefer7n500p_Tian_S4': 'Schaefer7n500p-IV Partial Correlation',\n",
    "\n",
    "'lifestyle-envir': 'Lifestyle & Environment',\n",
    "\n",
    "'allmri': '3 Brain MRI Modalities Stacked',\n",
    "'dwi': 'Brain dwMRI Stacked',\n",
    "'smri': 'Brain sMRI Stacked',\n",
    "'rs': 'Brain rsMRI Stacked',\n",
    "'body': 'Body Physiology Stacked',\n",
    "'body-comp': 'Body Composition Stacked',\n",
    "#'cardiopulmonary': 'Cardiopulmonary stacked',\n",
    "'renal-hepatic': 'Renal & Hepatic Stacked',\n",
    "'lifestyle-envir': 'Lifestyle & Environment',\n",
    "'brain-plus-body': '3 Brain MRI Modalities & Body Stacked',\n",
    "'brain-body': 'Brain & Body Stacked',\n",
    "'body-only': 'Body Physiology and Composition Stacked' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ea6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modalities\n",
    "modalities_body = [\n",
    "'immune',\n",
    "'renalhepatic',\n",
    "'metabolic',\n",
    "'cardiopulmonary',\n",
    "'musculoskeletal',\n",
    "'bone_densitometry',\n",
    "'pwa',\n",
    "'heart_mri',\n",
    "'carotid_ultrasound',\n",
    "'arterial_stiffness',\n",
    "'ecg_rest',\n",
    "'body_composition_by_impedance',\n",
    "'body_composition_dxa',\n",
    "'bone_dxa',\n",
    "'kidneys_mri',\n",
    "'liver_mri',\n",
    "'abdominal_composition_mri_18_vars', #17 vars\n",
    "'abdominal_organ_composition_mri_13_vars', #12 vars\n",
    "'hearing'\n",
    "]\n",
    "\n",
    "modalities_brain = [\n",
    "'struct_fast',\n",
    "'struct_sub_first',\n",
    "'struct_fs_aseg_mean_intensity',\n",
    "'struct_fs_aseg_volume',\n",
    "'struct_ba_exvivo_area', \n",
    "'struct_ba_exvivo_mean_thickness',\n",
    "'struct_ba_exvivo_volume',\n",
    "'struct_a2009s_area',\n",
    "'struct_a2009s_mean_thickness',\n",
    "'struct_a2009s_volume',\n",
    "'struct_dkt_area',\n",
    "'struct_dkt_mean_thickness',\n",
    "'struct_dkt_volume',\n",
    "'struct_desikan_gw',\n",
    "'struct_desikan_pial',\n",
    "'struct_desikan_white_area',\n",
    "'struct_desikan_white_mean_thickness',\n",
    "'struct_desikan_white_volume',\n",
    "'struct_subsegmentation',\n",
    "'add_t1',\n",
    "'add_t2',\n",
    "\n",
    "\"dwi_FA_tbss\", \"dwi_FA_prob\",\n",
    "\"dwi_MD_tbss\", \"dwi_MD_prob\",\n",
    "\"dwi_L1_tbss\", \"dwi_L1_prob\",\n",
    "\"dwi_L2_tbss\", \"dwi_L2_prob\",\n",
    "\"dwi_L3_tbss\", \"dwi_L3_prob\",\n",
    "\"dwi_MO_tbss\", \"dwi_MO_prob\",\n",
    "\"dwi_OD_tbss\", \"dwi_OD_prob\",\n",
    "\"dwi_ICVF_tbss\", \"dwi_ICVF_prob\",\n",
    "\"dwi_ISOVF_tbss\", \"dwi_ISOVF_prob\",\n",
    "\n",
    "'aparc_Tian_S1_FA_i2',\n",
    "'aparc_Tian_S1_Length_i2',\n",
    "'aparc_Tian_S1_SIFT2_FBC_i2',\n",
    "'aparc_Tian_S1_Streamline_Count_i2',\n",
    "\n",
    "'aparc_a2009s_Tian_S1_FA_i2',\n",
    "'aparc_a2009s_Tian_S1_Length_i2',\n",
    "'aparc_a2009s_Tian_S1_SIFT2_FBC_i2',\n",
    "'aparc_a2009s_Tian_S1_Streamline_Count_i2',\n",
    "\n",
    "'Glasser_Tian_S1_FA_i2',\n",
    "'Glasser_Tian_S1_Length_i2',\n",
    "'Glasser_Tian_S1_SIFT2_FBC_i2',\n",
    "'Glasser_Tian_S1_Streamline_Count_i2',\n",
    "\n",
    "'Glasser_Tian_S4_FA_i2',\n",
    "'Glasser_Tian_S4_Length_i2',\n",
    "'Glasser_Tian_S4_SIFT2_FBC_i2',\n",
    "'Glasser_Tian_S4_Streamline_Count_i2',\n",
    "\n",
    "'Schaefer7n200p_Tian_S1_FA_i2',\n",
    "'Schaefer7n200p_Tian_S1_Length_i2',\n",
    "'Schaefer7n200p_Tian_S1_SIFT2_FBC_i2',\n",
    "'Schaefer7n200p_Tian_S1_Streamline_Count_i2',\n",
    "\n",
    "'Schaefer7n1000p_Tian_S4_FA_i2',\n",
    "'Schaefer7n1000p_Tian_S4_Length_i2',\n",
    "'Schaefer7n1000p_Tian_S4_SIFT2_FBC_i2',\n",
    "'Schaefer7n1000p_Tian_S4_Streamline_Count_i2',\n",
    "\n",
    "\"amplitudes_21\",\n",
    "\"full_correlation_21\",\n",
    "\"partial_correlation_21\",\n",
    "\"amplitudes_55\",\n",
    "\"full_correlation_55\",\n",
    "\"partial_correlation_55\",\n",
    "'full_correlation_aparc_a2009s_Tian_S1',\n",
    "'full_correlation_aparc_Tian_S1',\n",
    "'full_correlation_Glasser_Tian_S1',\n",
    "'full_correlation_Glasser_Tian_S4',\n",
    "'full_correlation_Schaefer7n200p_Tian_S1',\n",
    "'full_correlation_Schaefer7n500p_Tian_S4',\n",
    "'partial_correlation_aparc_a2009s_Tian_S1',\n",
    "'partial_correlation_aparc_Tian_S1',\n",
    "'partial_correlation_Glasser_Tian_S1',\n",
    "'partial_correlation_Glasser_Tian_S4',\n",
    "'partial_correlation_Schaefer7n200p_Tian_S1',\n",
    "'partial_correlation_Schaefer7n500p_Tian_S4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that renamed columns based on modality map\n",
    "def clean_modality_name(name, prefix_to_remove=None):\n",
    "\n",
    "    clean_name = name\n",
    "    \n",
    "    # Remove specified prefix(es) if provided\n",
    "    if prefix_to_remove is not None:\n",
    "        if isinstance(prefix_to_remove, str):\n",
    "            # Single prefix\n",
    "            if clean_name.startswith(prefix_to_remove):\n",
    "                clean_name = clean_name.replace(prefix_to_remove, '', 1)\n",
    "        elif isinstance(prefix_to_remove, list):\n",
    "            # Multiple prefixes - remove each one that matches\n",
    "            for prefix in prefix_to_remove:\n",
    "                if clean_name.startswith(prefix):\n",
    "                    clean_name = clean_name.replace(prefix, '', 1)\n",
    "                    break  # Remove only the first matching prefix\n",
    "    \n",
    "    # Use modality_map for display name, fallback to original if not found\n",
    "    return modality_map.get(clean_name, clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from datetime import datetime\n",
    "folds = range(0, 5)\n",
    "base_path = '/UK_BB/brainbody'\n",
    "fig_path = '/UK_BB/brainbody/figures'\n",
    "\n",
    "# Define body modalities\n",
    "modalities_body = [\n",
    "    'immune',\n",
    "    'renalhepatic',\n",
    "    'metabolic',\n",
    "    'cardiopulmonary',\n",
    "    'musculoskeletal',\n",
    "    'bone_densitometry',\n",
    "    'pwa',\n",
    "    'heart_mri',\n",
    "    'carotid_ultrasound',\n",
    "    'arterial_stiffness',\n",
    "    'ecg_rest',\n",
    "    'body_composition_by_impedance',\n",
    "    'body_composition_dxa',\n",
    "    'bone_dxa',\n",
    "    'kidneys_mri',\n",
    "    'liver_mri',\n",
    "    'abdominal_composition_mri_18_vars',\n",
    "    'abdominal_organ_composition_mri_13_vars',\n",
    "    'hearing'\n",
    "]\n",
    "\n",
    "# Demographics confounds\n",
    "demo = pd.read_csv('/UK_BB/brainbody/demographics_full.csv')\n",
    "# Rename columns and count NAs\n",
    "df_demo_i2 = demo[[\n",
    "'eid',\n",
    "'31-0.0',\n",
    "'21000-0.0',\n",
    "'21003-2.0',]]\n",
    "demo_full = df_demo_i2.rename(columns={\n",
    "'31-0.0':'Sex',\n",
    "'21000-0.0':'Ethnicity',\n",
    "'21003-2.0':'Age',\n",
    "})\n",
    "\n",
    "age = demo_full['Age']\n",
    "#/media/hcs-sci-psy-narun\n",
    "sex = demo_full['Sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627b416",
   "metadata": {},
   "source": [
    "### Compute correlation between g-factors predicted from each modality with composite body marker\n",
    "\n",
    "*g pred stack CORR g pred each body modality*\n",
    "\n",
    "Supplementary S7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027298a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31897, 21)\n"
     ]
    }
   ],
   "source": [
    "# Combine g-factors predicted from the stacked body model across five folds\n",
    "g_pred_stacked = []\n",
    "for fold in folds:\n",
    "    try:\n",
    "        stacked_path = os.path.join(\n",
    "            base_path,\n",
    "            'stacking/body',\n",
    "            'folds',\n",
    "            f'fold_{fold}',\n",
    "            'g_pred',\n",
    "            f'body_target_pred_2nd_level_0_outer_test_fold_{fold}.csv'\n",
    "        )\n",
    "        df = pd.read_csv(stacked_path)\n",
    "        g_pred_stacked.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading stacked g-factor for fold {fold}: {str(e)}\")\n",
    "\n",
    "# Combine stacked predictions\n",
    "g_pred_stacked = pd.concat(g_pred_stacked, axis=0, ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Merge modality-specific g-factor predictions\n",
    "# -------------------------------------------------------------------\n",
    "modality_frames = []\n",
    "for modality in modalities_body:\n",
    "    modality_data = []\n",
    "    for fold in folds:\n",
    "        try:\n",
    "            modality_path = os.path.join(\n",
    "                base_path,\n",
    "                'lifestyle-envir-body',\n",
    "                'folds',\n",
    "                f'fold_{fold}',\n",
    "                'g_pred',\n",
    "                f'{modality}_g_pred_XGB_test_with_id_fold_{fold}.csv'\n",
    "            )\n",
    "            df = pd.read_csv(modality_path)\n",
    "            modality_data.append(df[['eid', f'g_pred_test_{modality}']])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {modality} for fold {fold}: {str(e)}\")\n",
    "    if modality_data:\n",
    "        combined_modality = pd.concat(modality_data, axis=0, ignore_index=True)\n",
    "        modality_frames.append(combined_modality)\n",
    "\n",
    "# First modality DataFrame\n",
    "merged_modalities = modality_frames[0]\n",
    "# Merge the rest one by one\n",
    "for df in modality_frames[1:]:\n",
    "    merged_modalities = pd.merge(merged_modalities, df, on='eid', how='outer')\n",
    "\n",
    "# Merge with stacked g-factor predictions\n",
    "combined = pd.merge(merged_modalities, g_pred_stacked[['eid', f'g_pred_stack_test']], on='eid', how='inner')\n",
    "combined.to_csv(os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', f'g_pred_from_body_mod_g_pred_stack_combined_{timestamp}.csv'), index=False)\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd73f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation analysis complete. Results saved to CSV and Excel.\n"
     ]
    }
   ],
   "source": [
    "# Compute correlations\n",
    "correlations = []\n",
    "for col in combined.columns:\n",
    "    if col not in ['eid', 'g_pred_stack_test']:\n",
    "        try:\n",
    "            x = combined['g_pred_stack_test']\n",
    "            y = combined[col]\n",
    "            mask = ~x.isna() & ~y.isna()\n",
    "            if mask.sum() >= 2:\n",
    "                r, p = stats.pearsonr(x[mask], y[mask])\n",
    "            else:\n",
    "                r, p = np.nan, np.nan\n",
    "            correlations.append({'Modality': col, 'Pearson r': r, 'p-value': p})\n",
    "        except Exception as e:\n",
    "            print(f\"Error correlating {col}: {str(e)}\")\n",
    "            correlations.append({'Modality': col, 'Pearson r': np.nan, 'p-value': np.nan})\n",
    "\n",
    "# Save results\n",
    "corr_results = pd.DataFrame(correlations).sort_values('Pearson r', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "corr_results.to_csv(os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', 'g_pred_from_body_mod_g_pred_stack_correlations.csv'), index=False)\n",
    "\n",
    "# Apply renaming function before saving to Excel\n",
    "corr_results['Modality'] = corr_results['Modality'].apply(\n",
    "    lambda x: clean_modality_name(x, prefix_to_remove='g_pred_test_')\n",
    ")\n",
    "\n",
    "# Save to Excel\n",
    "corr_results.to_excel(\n",
    "    os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', 'g_pred_from_body_mod_g_pred_stack_correlations.xlsx'),\n",
    "    index=False,\n",
    "    engine='openpyxl'\n",
    ")\n",
    "\n",
    "print(\"Correlation analysis complete. Results saved to CSV and Excel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248483bb",
   "metadata": {},
   "source": [
    "### Correlation between body features and g-factors predicted from each body modality\n",
    "\n",
    "*g pred each body modality CORR scaled body features*\n",
    "\n",
    "Supplementary S7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42620456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation between features and predicted g-factor predicted from each modality\n",
    "modality_results = {}\n",
    "unsorted_results = {}\n",
    "\n",
    "# Process each body modality\n",
    "for modality in modalities_body:\n",
    "    features, g_pred = [], []\n",
    "    \n",
    "    for fold in folds:\n",
    "        try:\n",
    "            # Path to predicted g-factor values (consistent location)\n",
    "            test_path = os.path.join(\n",
    "                base_path, \n",
    "                'lifestyle-envir-body',\n",
    "                'folds', \n",
    "                f'fold_{fold}', \n",
    "                'g_pred', \n",
    "                f'{modality}_g_pred_XGB_test_with_id_fold_{fold}.csv'\n",
    "            )\n",
    "            \n",
    "            # Special path handling for hearing\n",
    "            if modality == 'hearing':\n",
    "                features_path = os.path.join(\n",
    "                    base_path,\n",
    "                    'hearing-vision',  # Different path for hearing\n",
    "                    'folds',\n",
    "                    f'fold_{fold}',\n",
    "                    'scaling',\n",
    "                    f'{modality}_test_scaled_fold_{fold}.csv'\n",
    "                )\n",
    "            else:\n",
    "                # Standard path for other body modalities\n",
    "                features_path = os.path.join(\n",
    "                    base_path,\n",
    "                    'body',\n",
    "                    'folds',\n",
    "                    f'fold_{fold}',\n",
    "                    'scaling',\n",
    "                    f'{modality}_test_scaled_fold_{fold}.csv'\n",
    "                )\n",
    "\n",
    "            # Read data\n",
    "            g_pred_test = pd.read_csv(test_path)\n",
    "            features_corrected = pd.read_csv(features_path)\n",
    "            \n",
    "            features.append(features_corrected)\n",
    "            g_pred.append(g_pred_test)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing fold {fold} for {modality}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not features or not g_pred:\n",
    "        print(f\"Skipping {modality} - no valid data\")\n",
    "        continue\n",
    "    \n",
    "    # Concatenate and calculate correlations\n",
    "    features_concat = pd.concat(features, axis=0, ignore_index=True)\n",
    "    g_pred_concat = pd.concat(g_pred, axis=0, ignore_index=True)\n",
    "    \n",
    "    correlations = []\n",
    "    for feature in features_concat.columns:\n",
    "        try:\n",
    "            x = g_pred_concat[f'g_pred_test_{modality}'].values\n",
    "            y = features_concat[feature].values\n",
    "            mask = ~np.isnan(y)\n",
    "            \n",
    "            if sum(mask) >= 2:\n",
    "                r_pred, p_pred = stats.pearsonr(x[mask], y[mask])\n",
    "            else:\n",
    "                r_pred, p_pred = np.nan, np.nan\n",
    "                \n",
    "            correlations.append({\n",
    "                'Feature': feature,\n",
    "                'Pearson r': r_pred,\n",
    "                'p-value': p_pred\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating {feature} in {modality}: {str(e)}\")\n",
    "            correlations.append({\n",
    "                'Feature': feature,\n",
    "                'Pearson r': np.nan,\n",
    "                'p-value': np.nan\n",
    "            })\n",
    "    \n",
    "    # Store both unsorted and sorted versions\n",
    "    df = pd.DataFrame(correlations)\n",
    "    unsorted_results[modality] = df.copy()\n",
    "    modality_results[modality] = df.sort_values('Pearson r', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Save individual CSV\n",
    "    os.makedirs(os.path.join(base_path, 'feature_imp', 'feature_imp_body'), exist_ok=True)\n",
    "    output_path = os.path.join(base_path, f'feature_imp/feature_imp_body/{modality}_corr_with_g_pred.csv')\n",
    "    df.round(4).to_csv(output_path, index=False)\n",
    "    print(f\"Saved CSV for {modality}\")\n",
    "\n",
    "########################################################\n",
    "\n",
    "# Save Excel files for body modalities\n",
    "with pd.ExcelWriter(os.path.join(base_path, 'feature_imp/feature_imp_body', f'body_modalities_unsorted.xlsx'), \n",
    "                   engine='openpyxl') as writer:\n",
    "    for modality, df in unsorted_results.items():\n",
    "        try:\n",
    "            # Use original modality name for sheet name, truncate to 31 chars\n",
    "            sheet_name = str(modality)[:31]\n",
    "            df.round(4).to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {modality} to unsorted Excel: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "with pd.ExcelWriter(os.path.join(base_path, 'feature_imp/feature_imp_body', f'body_modalities_sorted.xlsx'),\n",
    "                   engine='openpyxl') as writer:\n",
    "    # Individual sorted sheets\n",
    "    for modality, df in modality_results.items():\n",
    "        try:\n",
    "            sheet_name = str(modality)[:31]\n",
    "            df.round(4).to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {modality} to sorted Excel: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combined sorted sheet with pretty names\n",
    "    try:\n",
    "        combined_sorted = pd.concat(\n",
    "            [df.assign(Modality=modality_names.get(modality, modality))  # Use get() with fallback\n",
    "             for modality, df in modality_results.items()],\n",
    "            axis=0, ignore_index=True\n",
    "        )\n",
    "        combined_sorted.round(4).to_excel(\n",
    "            writer, \n",
    "            sheet_name='All_body_sorted', \n",
    "            index=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating combined sheet: {str(e)}\")\n",
    "\n",
    "print(\"\\nProcessing complete! Created:\")\n",
    "print(f\"- body_modalities_unsorted.xlsx (original names)\")\n",
    "print(f\"- body_modalities_sorted.xlsx (sorted with combined 'All_body_sorted' sheet)\")\n",
    "\n",
    "print(\"\\nProcessing complete! Created:\")\n",
    "print(f\"- Individual CSV files for each body modality in 'feature_imp_body' folder\")\n",
    "print(f\"- Excel with original names, unsorted results\")\n",
    "print(f\"- Excel with original names (sorted) + combined pretty-named sorted sheet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835d84b",
   "metadata": {},
   "source": [
    "### Correlation between body features and composite body marker\n",
    "\n",
    "*g pred body stack CORR scaled body features*\n",
    "\n",
    "Supplementary S8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2c690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that renamed columns based on modality map\n",
    "def clean_modality_name(name, prefix_to_remove=None):\n",
    "\n",
    "    clean_name = name\n",
    "    \n",
    "    # Remove specified prefix(es) if provided\n",
    "    if prefix_to_remove is not None:\n",
    "        if isinstance(prefix_to_remove, str):\n",
    "            # Single prefix\n",
    "            if clean_name.startswith(prefix_to_remove):\n",
    "                clean_name = clean_name.replace(prefix_to_remove, '', 1)\n",
    "        elif isinstance(prefix_to_remove, list):\n",
    "            # Multiple prefixes - remove each one that matches\n",
    "            for prefix in prefix_to_remove:\n",
    "                if clean_name.startswith(prefix):\n",
    "                    clean_name = clean_name.replace(prefix, '', 1)\n",
    "                    break  # Remove only the first matching prefix\n",
    "    \n",
    "    # Use modality_map for display name, fallback to original if not found\n",
    "    return modality_map.get(clean_name, clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77a150d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine g-factors predicted from the stacked body model across five folds\n",
    "g_pred_stacked = []\n",
    "for fold in folds:\n",
    "    try:\n",
    "        stacked_path = os.path.join(\n",
    "            base_path,\n",
    "            'stacking/body',\n",
    "            'folds',\n",
    "            f'fold_{fold}',\n",
    "            'g_pred',\n",
    "            f'body_target_pred_2nd_level_0_outer_test_fold_{fold}.csv'\n",
    "        )\n",
    "        df = pd.read_csv(stacked_path)\n",
    "        g_pred_stacked.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading stacked g-factor for fold {fold}: {str(e)}\")\n",
    "\n",
    "# Combine stacked predictions\n",
    "g_pred_stacked = pd.concat(g_pred_stacked, axis=0, ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Pool scaled features and merge with eid from pre-scaled files\n",
    "all_features = []\n",
    "for modality in modalities_body:\n",
    "    modality_features = []\n",
    "    for fold in folds:\n",
    "        try:\n",
    "            # Load scaled features\n",
    "            if modality == 'hearing':\n",
    "                scaled_path = os.path.join(\n",
    "                    base_path,\n",
    "                    'hearing-vision',\n",
    "                    'folds',\n",
    "                    f'fold_{fold}',\n",
    "                    'scaling',\n",
    "                    f'{modality}_test_scaled_fold_{fold}.csv'\n",
    "                )\n",
    "                prescaled_path = os.path.join(\n",
    "                    base_path,\n",
    "                    'hearing-vision',\n",
    "                    'folds',\n",
    "                    f'fold_{fold}',\n",
    "                    'suppl',\n",
    "                    f'{modality}_test_feat_targ_fold_{fold}.csv'\n",
    "                )\n",
    "            else:\n",
    "                scaled_path = os.path.join(\n",
    "                    base_path,\n",
    "                    'body',\n",
    "                    'folds',\n",
    "                    f'fold_{fold}',\n",
    "                    'scaling',\n",
    "                    f'{modality}_test_scaled_fold_{fold}.csv'\n",
    "                )\n",
    "                prescaled_path = os.path.join(\n",
    "                    base_path,\n",
    "                    'body',\n",
    "                    'folds',\n",
    "                    f'fold_{fold}',\n",
    "                    'suppl',\n",
    "                    f'{modality}_test_feat_targ_fold_{fold}.csv'\n",
    "                )\n",
    "\n",
    "            scaled_df = pd.read_csv(scaled_path)\n",
    "            prescaled_df = pd.read_csv(prescaled_path)\n",
    "\n",
    "            # Merge eid with scaled features\n",
    "            merged_df = pd.concat([prescaled_df[['eid']], scaled_df], axis=1)\n",
    "            modality_features.append(merged_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {modality}, fold {fold}: {str(e)}\")\n",
    "    if modality_features:\n",
    "        combined_modality = pd.concat(modality_features, axis=0, ignore_index=True)\n",
    "        all_features.append(combined_modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a696c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Identifying duplicate features ===\n",
      "immune: 32 features\n",
      "renalhepatic: 16 features\n",
      "metabolic: 14 features\n",
      "cardiopulmonary: 7 features\n",
      "musculoskeletal: 13 features\n",
      "bone_densitometry: 4 features\n",
      "pwa: 18 features\n",
      "heart_mri: 8 features\n",
      "carotid_ultrasound: 12 features\n",
      "arterial_stiffness: 4 features\n",
      "ecg_rest: 9 features\n",
      "body_composition_by_impedance: 32 features\n",
      "body_composition_dxa: 43 features\n",
      "bone_dxa: 68 features\n",
      "kidneys_mri: 4 features\n",
      "liver_mri: 2 features\n",
      "abdominal_composition_mri_18_vars: 17 features\n",
      "abdominal_organ_composition_mri_13_vars: 12 features\n",
      "hearing: 2 features\n",
      "\n",
      "Found 4 features in multiple modalities:\n",
      "  'Pulse rate': ['cardiopulmonary', 'arterial_stiffness']\n",
      "  'Weight': ['musculoskeletal', 'body_composition_by_impedance']\n",
      "  'Body mass index (BMI)': ['musculoskeletal', 'body_composition_by_impedance']\n",
      "  'Trunk fat mass': ['body_composition_by_impedance', 'body_composition_dxa']\n"
     ]
    }
   ],
   "source": [
    "# Identify duplicate feature names across all modalities\n",
    "print(\"=== Identifying duplicate features ===\")\n",
    "\n",
    "# Collect all feature names (excluding 'eid') from each modality\n",
    "all_feature_names = {}\n",
    "for i, df in enumerate(all_features):\n",
    "    modality = modalities_body[i]\n",
    "    features = [col for col in df.columns if col != 'eid']\n",
    "    all_feature_names[modality] = set(features)\n",
    "    print(f\"{modality}: {len(features)} features\")\n",
    "\n",
    "# Find features that appear in multiple modalities\n",
    "feature_counts = {}\n",
    "for modality, features in all_feature_names.items():\n",
    "    for feature in features:\n",
    "        if feature not in feature_counts:\n",
    "            feature_counts[feature] = []\n",
    "        feature_counts[feature].append(modality)\n",
    "\n",
    "# Show duplicate features\n",
    "duplicate_features = {feature: mods for feature, mods in feature_counts.items() if len(mods) > 1}\n",
    "print(f\"\\nFound {len(duplicate_features)} features in multiple modalities:\")\n",
    "for feature, mods in duplicate_features.items():\n",
    "    print(f\"  '{feature}': {mods}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096047b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ' (Cardiopulmonary)' to 1 features in cardiopulmonary\n",
      "Added ' (Musculoskeletal)' to 2 features in musculoskeletal\n",
      "Added ' (Arterial Stiffness)' to 1 features in arterial_stiffness\n",
      "Added ' (Impedance)' to 3 features in body_composition_by_impedance\n",
      "Added ' (DXA)' to 1 features in body_composition_dxa\n",
      "\n",
      "=== Checking FINAL merged dataframe ===\n",
      "Total columns in features_all_modalities: 318\n",
      "Found 9 renamed columns in features_all_modalities:\n",
      "  Pulse rate (Cardiopulmonary)\n",
      "  Body mass index (BMI) (Musculoskeletal)\n",
      "  Weight (Musculoskeletal)\n",
      "  Pulse rate (Arterial Stiffness)\n",
      "  Weight (Impedance)\n",
      "  Body mass index (BMI) (Impedance)\n",
      "  Trunk fat mass (Impedance)\n",
      "  Trunk fat mass (DXA)\n",
      "  Weight-to-muscle ratio\n",
      "Original duplicate names still in dataframe: []\n"
     ]
    }
   ],
   "source": [
    "# Define which features need modality suffixes (for duplicates)\n",
    "modality_suffix_map = {\n",
    "    'body_composition_by_impedance': ' (Impedance)',\n",
    "    'body_composition_dxa': ' (DXA)', \n",
    "    'musculoskeletal': ' (Musculoskeletal)',\n",
    "    'cardiopulmonary': ' (Cardiopulmonary)',\n",
    "    'arterial_stiffness': ' (Arterial Stiffness)'\n",
    "}\n",
    "\n",
    "# List of features that appear in multiple modalities\n",
    "duplicate_features = ['Trunk fat mass', 'Body mass index (BMI)', 'Weight', 'Pulse rate']\n",
    "\n",
    "# Add suffixes to duplicate features\n",
    "for i, df in enumerate(all_features):\n",
    "    modality = modalities_body[i]\n",
    "    if modality in modality_suffix_map:\n",
    "        suffix = modality_suffix_map[modality]\n",
    "        rename_dict = {}\n",
    "        for col in df.columns:\n",
    "            if col != 'eid' and col in duplicate_features:\n",
    "                rename_dict[col] = f\"{col}{suffix}\"\n",
    "        if rename_dict:\n",
    "            all_features[i] = df.rename(columns=rename_dict)\n",
    "            print(f\"Added '{suffix}' to {len(rename_dict)} features in {modality}\")\n",
    "\n",
    "# Merge all modalities column-wise on 'eid' without _x/_y suffixes for duplicates\n",
    "features_all_modalities = all_features[0]\n",
    "for df in all_features[1:]:\n",
    "    features_all_modalities = pd.merge(features_all_modalities, df, on='eid', how='outer')\n",
    "\n",
    "print(\"\\n=== Checking FINAL merged dataframe ===\")\n",
    "print(f\"Total columns in features_all_modalities: {len(features_all_modalities.columns)}\")\n",
    "\n",
    "# Check if renamed columns exist in the final dataframe\n",
    "renamed_in_final = [col for col in features_all_modalities.columns if any(feature in col for feature in duplicate_features)]\n",
    "print(f\"Found {len(renamed_in_final)} renamed columns in features_all_modalities:\")\n",
    "for col in renamed_in_final:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Check if any original duplicate names still exist\n",
    "original_duplicates_in_final = [col for col in features_all_modalities.columns if col in duplicate_features]\n",
    "print(f\"Original duplicate names still in dataframe: {original_duplicates_in_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebe70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features with stacked g-factor predictions on 'eid'\n",
    "combined = pd.merge(features_all_modalities, g_pred_stacked[['eid', 'g_pred_stack_test']], on='eid', how='inner')\n",
    "\n",
    "# Save the final combined DataFrame\n",
    "output_path = os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', f'body_features_g_pred_stack_combined.csv')\n",
    "combined.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Final combined shape: {combined.shape}\")\n",
    "\n",
    "# Check if the column exists\n",
    "if 'Body mass index (BMI) (Musculoskeletal)' in combined.columns:\n",
    "    print(\"Column exists! Here are the first few values:\")\n",
    "    print(combined['Body mass index (BMI) (Musculoskeletal)'].head())\n",
    "else:\n",
    "    print(\"Column NOT found. Available columns with 'BMI':\")\n",
    "    bmi_cols = [col for col in combined.columns if 'BMI' in col]\n",
    "    for col in bmi_cols:\n",
    "        print(f\"  '{col}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b6813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation analysis complete. Results saved to CSV and Excel.\n"
     ]
    }
   ],
   "source": [
    "# Compute correlations with stacked g-factor\n",
    "correlations = []\n",
    "for col in combined.columns:\n",
    "    if col not in ['eid', 'g_pred_stack_test']:\n",
    "        try:\n",
    "            x = combined['g_pred_stack_test']\n",
    "            y = combined[col]\n",
    "            mask = ~x.isna() & ~y.isna()\n",
    "            if mask.sum() >= 2:\n",
    "                r, p = stats.pearsonr(x[mask], y[mask])\n",
    "            else:\n",
    "                r, p = np.nan, np.nan\n",
    "            correlations.append({'Phenotype': col, 'Pearson r': r, 'p-value': p})\n",
    "        except Exception as e:\n",
    "            print(f\"Error correlating {col}: {str(e)}\")\n",
    "            correlations.append({'Phenotype': col, 'Pearson r': np.nan, 'p-value': np.nan})\n",
    "\n",
    "# Save results\n",
    "corr_results = pd.DataFrame(correlations).sort_values('Pearson r', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "corr_results.to_csv(os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', 'body_features_g_pred_stack_correlations.csv'), index=False)\n",
    "\n",
    "# Apply renaming function before saving to Excel\n",
    "corr_results['Phenotype'] = corr_results['Phenotype'].apply(\n",
    "    lambda x: clean_modality_name(x, prefix_to_remove='g_pred_test_')\n",
    ")\n",
    "\n",
    "# Save to Excel\n",
    "corr_results.to_excel(\n",
    "    os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', 'body_features_g_pred_stack_correlations.xlsx'),\n",
    "    index=False,\n",
    "    engine='openpyxl'\n",
    ")\n",
    "\n",
    "print(\"Correlation analysis complete. Results saved to CSV and Excel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6760d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of phenotypes analyzed: 317\n",
      "Number of valid tests for Bonferroni correction: 317\n",
      "Number of valid male tests for Bonferroni correction: 317\n",
      "Number of valid female tests for Bonferroni correction: 317\n",
      "Average sample size: 24895\n",
      "Total missing values across all phenotypes: 2,219,720\n",
      "\n",
      "Sample size range: 17492 - 30971\n",
      "Male sample size range: 8441 - 15029\n",
      "Female sample size range: 8482 - 15942\n",
      "Total rows in combined: 31897\n",
      "Total rows in demo: 502356\n",
      "Rows after merge: 31897\n",
      "\n",
      "Missing Sex values in combined_with_demo: 0\n",
      "Unique Sex values: [0 1]\n",
      "\n",
      "Missing in g_pred_stack_test: 0\n",
      "Correlation analysis complete. Results saved to CSV and Excel with multiple sheets.\n",
      "Excel file contains: All_correlations, Male_correlations, Female_correlations, Summary_stats sheets\n"
     ]
    }
   ],
   "source": [
    "# Stratify analysis by sex\n",
    "demo = demo_full.copy()\n",
    "\n",
    "# Merge demographics with combined data\n",
    "combined_with_demo = pd.merge(combined, demo[['eid', 'Sex']], on='eid', how='left')\n",
    "\n",
    "# Compute correlations with stacked g-factor\n",
    "correlations = []\n",
    "for col in combined.columns:\n",
    "    if col not in ['eid', 'g_pred_stack_test']:\n",
    "        try:\n",
    "            x = combined['g_pred_stack_test']\n",
    "            y = combined[col]\n",
    "            \n",
    "            # Create mask for complete cases\n",
    "            mask = ~x.isna() & ~y.isna()\n",
    "            sample_size = mask.sum()\n",
    "            missing_count = len(x) - sample_size\n",
    "            \n",
    "            if sample_size >= 2:\n",
    "                # Overall correlation\n",
    "                r, p = stats.pearsonr(x[mask], y[mask])\n",
    "                \n",
    "                # Male-only correlation\n",
    "                #1\tMale\n",
    "                #0\tFemale\n",
    "                mask_male = mask & (combined_with_demo['Sex'] == 1)  # 1 = male\n",
    "                sample_size_male = mask_male.sum()\n",
    "                if sample_size_male >= 2:\n",
    "                    r_male, p_male = stats.pearsonr(x[mask_male], y[mask_male])\n",
    "                else:\n",
    "                    r_male, p_male = np.nan, np.nan\n",
    "                \n",
    "                # Female-only correlation  \n",
    "                mask_female = mask & (combined_with_demo['Sex'] == 0)  # 0 = female\n",
    "                sample_size_female = mask_female.sum()\n",
    "                if sample_size_female >= 2:\n",
    "                    r_female, p_female = stats.pearsonr(x[mask_female], y[mask_female])\n",
    "                else:\n",
    "                    r_female, p_female = np.nan, np.nan\n",
    "                    \n",
    "            else:\n",
    "                r, p = np.nan, np.nan\n",
    "                r_male, p_male = np.nan, np.nan\n",
    "                r_female, p_female = np.nan, np.nan\n",
    "                sample_size_male = np.nan\n",
    "                sample_size_female = np.nan\n",
    "                \n",
    "            correlations.append({\n",
    "                'Phenotype': col, \n",
    "                'Pearson r': r, \n",
    "                'p-value': p,\n",
    "                'N': sample_size,\n",
    "                'N missing': missing_count,\n",
    "                'Pearson r male': r_male,\n",
    "                'p-value male': p_male,\n",
    "                'N male': sample_size_male,\n",
    "                'Pearson r female': r_female, \n",
    "                'p-value female': p_female,\n",
    "                'N female': sample_size_female\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error correlating {col}: {str(e)}\")\n",
    "            correlations.append({\n",
    "                'Phenotype': col, \n",
    "                'Pearson r': np.nan, \n",
    "                'p-value': np.nan,\n",
    "                'N': np.nan,\n",
    "                'N missing': np.nan,\n",
    "                'Pearson r male': np.nan,\n",
    "                'p-value male': np.nan,\n",
    "                'N male': np.nan,\n",
    "                'Pearson r female': np.nan,\n",
    "                'p-value female': np.nan,\n",
    "                'N female': np.nan\n",
    "            })\n",
    "\n",
    "# Save results\n",
    "corr_results = pd.DataFrame(correlations)#.sort_values('Pearson r', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Calculate number of valid tests for Bonferroni correction\n",
    "valid_tests = corr_results['p-value'].notna().sum()\n",
    "valid_tests_male = corr_results['p-value male'].notna().sum()\n",
    "valid_tests_female = corr_results['p-value female'].notna().sum()\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "corr_results['p-value bonferroni'] = corr_results['p-value'].apply(\n",
    "    lambda x: min(x * valid_tests, 1.0) if not pd.isna(x) else np.nan\n",
    ")\n",
    "corr_results['p-value male bonferroni'] = corr_results['p-value male'].apply(\n",
    "    lambda x: min(x * valid_tests_male, 1.0) if not pd.isna(x) else np.nan\n",
    ")\n",
    "corr_results['p-value female bonferroni'] = corr_results['p-value female'].apply(\n",
    "    lambda x: min(x * valid_tests_female, 1.0) if not pd.isna(x) else np.nan\n",
    ")\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Total number of phenotypes analyzed: {len(corr_results)}\")\n",
    "print(f\"Number of valid tests for Bonferroni correction: {valid_tests}\")\n",
    "print(f\"Number of valid male tests for Bonferroni correction: {valid_tests_male}\")\n",
    "print(f\"Number of valid female tests for Bonferroni correction: {valid_tests_female}\")\n",
    "print(f\"Average sample size: {corr_results['N'].mean():.0f}\")\n",
    "print(f\"Total missing values across all phenotypes: {corr_results['N missing'].sum():,}\")\n",
    "\n",
    "# Print sample size ranges\n",
    "print(f\"\\nSample size range: {corr_results['N'].min():.0f} - {corr_results['N'].max():.0f}\")\n",
    "print(f\"Male sample size range: {corr_results['N male'].min():.0f} - {corr_results['N male'].max():.0f}\")\n",
    "print(f\"Female sample size range: {corr_results['N female'].min():.0f} - {corr_results['N female'].max():.0f}\")\n",
    "\n",
    "corr_results_sorted = pd.DataFrame(corr_results).sort_values('Pearson r', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "corr_results.to_csv(os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', 'body_features_g_pred_stack_correlations_detailed.csv'), index=False)\n",
    "corr_results_sorted.to_csv(os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', 'body_features_g_pred_stack_correlations_detailed_sorted.csv'), index=False)\n",
    "\n",
    "# Apply renaming function before saving to Excel\n",
    "corr_results['Phenotype'] = corr_results['Phenotype'].apply(\n",
    "    lambda x: clean_modality_name(x, prefix_to_remove='g_pred_test_')\n",
    ")\n",
    "\n",
    "# Save to Excel with multiple sheets\n",
    "with pd.ExcelWriter(\n",
    "    os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', 'body_features_g_pred_stack_correlations_detailed.xlsx'),\n",
    "    engine='openpyxl'\n",
    ") as writer:\n",
    "    \n",
    "    # Main results\n",
    "    corr_results.to_excel(writer, sheet_name='All_correlations', index=False)\n",
    "    \n",
    "    # Male-only results\n",
    "    male_results = corr_results[['Phenotype', 'Pearson r male', 'p-value male', 'p-value male bonferroni', 'N male']].copy()\n",
    "    male_results = male_results.sort_values('Pearson r male', ascending=False)\n",
    "    male_results.to_excel(writer, sheet_name='Male_correlations', index=False)\n",
    "    \n",
    "    # Female-only results  \n",
    "    female_results = corr_results[['Phenotype', 'Pearson r female', 'p-value female', 'p-value female bonferroni', 'N female']].copy()\n",
    "    female_results = female_results.sort_values('Pearson r female', ascending=False)\n",
    "    female_results.to_excel(writer, sheet_name='Female_correlations', index=False)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary_data = {\n",
    "        'Metric': ['Total phenotypes', 'Valid tests for Bonferroni', 'Valid male tests', 'Valid female tests',\n",
    "                  'Mean sample size', 'Total missing values',\n",
    "                  'Min sample size', 'Max sample size',\n",
    "                  'Min male sample size', 'Max male sample size',\n",
    "                  'Min female sample size', 'Max female sample size'],\n",
    "        'Value': [len(corr_results), valid_tests, valid_tests_male, valid_tests_female,\n",
    "                 corr_results['N'].mean(), corr_results['N missing'].sum(),\n",
    "                 corr_results['N'].min(), corr_results['N'].max(),\n",
    "                 corr_results['N male'].min(), corr_results['N male'].max(),\n",
    "                 corr_results['N female'].min(), corr_results['N female'].max()]\n",
    "    }\n",
    "\n",
    "    # Check for missing values\n",
    "    print(f\"Total rows in combined: {len(combined)}\")\n",
    "    print(f\"Total rows in demo: {len(demo)}\")\n",
    "    print(f\"Rows after merge: {len(combined_with_demo)}\")\n",
    "    \n",
    "    # Check missing sex information\n",
    "    print(f\"\\nMissing Sex values in combined_with_demo: {combined_with_demo['Sex'].isna().sum()}\")\n",
    "    print(f\"Unique Sex values: {combined_with_demo['Sex'].dropna().unique()}\")\n",
    "    \n",
    "    # Check for NaN in g_pred_stack_test\n",
    "    print(f\"\\nMissing in g_pred_stack_test: {combined['g_pred_stack_test'].isna().sum()}\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary_stats', index=False)\n",
    "\n",
    "# Save to Excel with multiple sheets - sorted\n",
    "with pd.ExcelWriter(\n",
    "    os.path.join(base_path, 'feature_imp', 'feature_imp_body', 'combined', 'body_features_g_pred_stack_correlations_detailed_sorted.xlsx'),\n",
    "    engine='openpyxl'\n",
    ") as writer:\n",
    "    \n",
    "    # Main results\n",
    "    corr_results_sorted.to_excel(writer, sheet_name='All_correlations', index=False)\n",
    "    \n",
    "    # Male-only results\n",
    "    male_results = corr_results_sorted[['Phenotype', 'Pearson r male', 'p-value male', 'p-value male bonferroni', 'N male']].copy()\n",
    "    male_results = male_results.sort_values('Pearson r male', ascending=False)\n",
    "    male_results.to_excel(writer, sheet_name='Male_correlations', index=False)\n",
    "    \n",
    "    # Female-only results  \n",
    "    female_results = corr_results_sorted[['Phenotype', 'Pearson r female', 'p-value female', 'p-value female bonferroni', 'N female']].copy()\n",
    "    female_results = female_results.sort_values('Pearson r female', ascending=False)\n",
    "    female_results.to_excel(writer, sheet_name='Female_correlations', index=False)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary_data = {\n",
    "        'Metric': ['Total phenotypes', 'Valid tests for Bonferroni', 'Valid male tests', 'Valid female tests',\n",
    "                  'Mean sample size', 'Total missing values',\n",
    "                  'Min sample size', 'Max sample size',\n",
    "                  'Min male sample size', 'Max male sample size',\n",
    "                  'Min female sample size', 'Max female sample size'],\n",
    "        'Value': [len(corr_results_sorted), valid_tests, valid_tests_male, valid_tests_female,\n",
    "                 corr_results_sorted['N'].mean(), corr_results_sorted['N missing'].sum(),\n",
    "                 corr_results_sorted['N'].min(), corr_results_sorted['N'].max(),\n",
    "                 corr_results_sorted['N male'].min(), corr_results_sorted['N male'].max(),\n",
    "                 corr_results_sorted['N female'].min(), corr_results_sorted['N female'].max()]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary_stats', index=False)\n",
    "\n",
    "\n",
    "print(\"Correlation analysis complete. Results saved to CSV and Excel with multiple sheets.\")\n",
    "print(f\"Excel file contains: All_correlations, Male_correlations, Female_correlations, Summary_stats sheets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
